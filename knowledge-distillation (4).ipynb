{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# FILE: 01_baseline.py\nimport pandas as pd\nimport numpy as np\nfrom datasets import load_dataset, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score\nimport joblib\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    AutoModelForSequenceClassification,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n    get_linear_schedule_with_warmup\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom accelerate import Accelerator\nfrom tqdm import tqdm\nimport os\nimport time\n\nprint(\"=\"*80)\nprint(\"STEP 1: TRAINING BASELINE MODEL (TF-IDF + NAIVE BAYES)\")\nprint(\"=\"*80)\n\n# --- DATA LOADING ---\nprint(\"Loading IMDB dataset...\")\ndataset = load_dataset(\"imdb\")\ndf_train = dataset['train'].to_pandas()\ndf_test = dataset['test'].to_pandas()\nprint(f\"Loaded {len(df_train)} training samples and {len(df_test)} test samples.\")\n\nX_train, y_train = df_train['text'], df_train['label']\nX_test, y_test = df_test['text'], df_test['label']\n\n# --- MODEL TRAINING ---\nbaseline_model = Pipeline([\n    ('tfidf', TfidfVectorizer(stop_words='english', max_features=20000, ngram_range=(1, 2))),\n    ('classifier', MultinomialNB(alpha=0.1))\n])\n\nprint(\"\\nTraining the baseline model...\")\nbaseline_model.fit(X_train, y_train)\nprint(\"Training complete.\")\n\n# --- EVALUATION ---\nprint(\"\\nEvaluating baseline model on the test set...\")\ny_pred = baseline_model.predict(X_test)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['NEGATIVE', 'POSITIVE']))\n\naccuracy = baseline_model.score(X_test, y_test)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# --- SAVING THE MODEL ---\noutput_dir = \"./models\"\nos.makedirs(output_dir, exist_ok=True)\nmodel_path = os.path.join(output_dir, \"baseline_model.joblib\")\njoblib.dump(baseline_model, model_path)\nprint(f\"\\n✅ Baseline model saved to: {model_path}\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T15:07:55.936430Z","iopub.execute_input":"2025-12-02T15:07:55.936694Z","iopub.status.idle":"2025-12-02T15:09:01.548320Z","shell.execute_reply.started":"2025-12-02T15:07:55.936674Z","shell.execute_reply":"2025-12-02T15:09:01.547647Z"}},"outputs":[{"name":"stderr","text":"2025-12-02 15:08:07.545284: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764688087.732526     109 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764688087.786635     109 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"================================================================================\nSTEP 1: TRAINING BASELINE MODEL (TF-IDF + NAIVE BAYES)\n================================================================================\nLoading IMDB dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a581f989f07e40b88af196ca128b973c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c6b08d98ca5409aa3d2b54c8355f1af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c60306ad5b31416f900fd933e1d5a350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/unsupervised-00000-of-00001.p(…):   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bda5a664ded4345bafd6b36e44cec8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d42fc5a5d08446ba2fa9bbca9c8286b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e180303d9d504a769c2339dfcceeefa3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"266c23ef068c4f368cde868d72c2450a"}},"metadata":{}},{"name":"stdout","text":"Loaded 25000 training samples and 25000 test samples.\n\nTraining the baseline model...\nTraining complete.\n\nEvaluating baseline model on the test set...\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    NEGATIVE       0.83      0.87      0.85     12500\n    POSITIVE       0.86      0.83      0.84     12500\n\n    accuracy                           0.85     25000\n   macro avg       0.85      0.85      0.85     25000\nweighted avg       0.85      0.85      0.85     25000\n\nTest Accuracy: 0.8470\n\n✅ Baseline model saved to: ./models/baseline_model.joblib\n================================================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#!pip install --upgrade transformers huggingface_hub accelerate peft bitsandbytes accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T15:09:01.549577Z","iopub.execute_input":"2025-12-02T15:09:01.550378Z","iopub.status.idle":"2025-12-02T15:09:01.553626Z","shell.execute_reply.started":"2025-12-02T15:09:01.550358Z","shell.execute_reply":"2025-12-02T15:09:01.552862Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# FILE: 02_finetune_roberta.py\nprint(\"=\" * 80)\nprint(\"STEP 2: FINE-TUNING ROBERTA-LARGE TEACHER MODEL\")\nprint(\"=\" * 80)\n\n# --- CONFIGURATION ---\nclass Config:\n    MODEL_NAME = \"roberta-large\"\n    MAX_LENGTH = 256\n    RANDOM_SEED = 42\n    BATCH_SIZE = 16\n    GRADIENT_ACCUMULATION = 2\n    NUM_EPOCHS = 2\n    LEARNING_RATE = 2e-5\n    OUTPUT_DIR = \"./models/teacher_roberta\"\n    LOGITS_OUTPUT_PATH = \"./data/roberta_logits.csv\"\n\nconfig = Config()\nos.makedirs(\"./data\", exist_ok=True)\n\n# --- DATA PREPARATION ---\nprint(\"Loading and preparing IMDB dataset...\")\ndataset = load_dataset(\"imdb\")\ntrain_data = dataset['train']\n# Use a subset of the test set for validation during training\ntest_val_split = dataset['test'].train_test_split(test_size=0.5, seed=config.RANDOM_SEED)\nval_data = test_val_split['train']\ntest_data = test_val_split['test']\nprint(f\"Train: {len(train_data)}, Validation: {len(val_data)}, Test: {len(test_data)}\")\n\n# --- MODEL AND TOKENIZER ---\nprint(f\"Loading model and tokenizer: {config.MODEL_NAME}\")\ntokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(config.MODEL_NAME, num_labels=2)\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, max_length=config.MAX_LENGTH)\n\ntrain_dataset = train_data.map(tokenize_function, batched=True)\nval_dataset = val_data.map(tokenize_function, batched=True)\ntest_dataset = test_data.map(tokenize_function, batched=True)\n\n# --- TRAINING ---\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\"accuracy\": accuracy_score(labels, predictions)}\n\ntraining_args = TrainingArguments(\n    output_dir=config.OUTPUT_DIR,\n    num_train_epochs=config.NUM_EPOCHS,\n    per_device_train_batch_size=config.BATCH_SIZE,\n    per_device_eval_batch_size=config.BATCH_SIZE,\n    gradient_accumulation_steps=config.GRADIENT_ACCUMULATION,\n    learning_rate=config.LEARNING_RATE,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    fp16=True,\n    report_to=\"none\",\n    seed=config.RANDOM_SEED,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    data_collator=DataCollatorWithPadding(tokenizer),\n)\n\nprint(\"Starting RoBERTa fine-tuning...\")\ntrainer.train()\nprint(\"Fine-tuning complete.\")\n\n# --- EVALUATION AND SAVING ---\nif trainer.is_world_process_zero():\n    print(\"\\nEvaluating on test set...\")\n    test_results = trainer.evaluate(test_dataset)\n    print(f\"  Test Accuracy: {test_results['eval_accuracy']:.4f}\")\n\n    print(\"\\nSaving model...\")\n    trainer.save_model(config.OUTPUT_DIR)\n    tokenizer.save_pretrained(config.OUTPUT_DIR)\n    print(f\"  ✓ Model saved to: {config.OUTPUT_DIR}\")\n\n    print(\"\\nGenerating and saving soft labels for distillation...\")\n    train_preds = trainer.predict(train_dataset)\n    \n    df_logits = pd.DataFrame({\n        'label': train_preds.label_ids,\n        'logit_0': train_preds.predictions[:, 0],\n        'logit_1': train_preds.predictions[:, 1],\n    })\n    df_logits.to_csv(config.LOGITS_OUTPUT_PATH, index=False)\n    print(f\"  ✓ Soft labels saved to: {config.LOGITS_OUTPUT_PATH}\")\n\nprint(\"\\n✅ RoBERTa teacher preparation complete!\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T15:09:01.554471Z","iopub.execute_input":"2025-12-02T15:09:01.554841Z","iopub.status.idle":"2025-12-02T16:57:22.411640Z","shell.execute_reply.started":"2025-12-02T15:09:01.554824Z","shell.execute_reply":"2025-12-02T16:57:22.410946Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nSTEP 2: FINE-TUNING ROBERTA-LARGE TEACHER MODEL\n================================================================================\nLoading and preparing IMDB dataset...\nTrain: 25000, Validation: 12500, Test: 12500\nLoading model and tokenizer: roberta-large\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc2fb0ef9f884f2783c2db3f1ccd2bba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8f2436a88b8420c889d1e761a48197c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d198bbab3c4dd789e1130cef98cb04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02548119391a4170ba9c233bf55d59e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e48befacff34b559fc62294123f6897"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ead7cd9f2deb45ff817875b6857450ba"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"686f7eb88f8f4ced84b4e25c98d9d1a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02653043578b41399ed90bc16fafcd44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dd6b381d05d40fa8a7c8e6037d5152f"}},"metadata":{}},{"name":"stdout","text":"Starting RoBERTa fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [782/782 1:28:21, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.143336</td>\n      <td>0.943920</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.181200</td>\n      <td>0.142743</td>\n      <td>0.952080</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Fine-tuning complete.\n\nEvaluating on test set...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"  Test Accuracy: 0.9536\n\nSaving model...\n  ✓ Model saved to: ./models/teacher_roberta\n\nGenerating and saving soft labels for distillation...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  ✓ Soft labels saved to: ./data/roberta_logits.csv\n\n✅ RoBERTa teacher preparation complete!\n================================================================================\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# FILE: 3_finetuning_mistral.py\nimport torch\nimport gc\nimport os\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    BitsAndBytesConfig, \n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\n\n# --- MEMORY CLEANUP START ---\ngc.collect()\ntorch.cuda.empty_cache()\n# ----------------------------\n\nprint(\"=\" * 80)\nprint(\"STEP 3: FINE-TUNING MISTRAL (MEMORY OPTIMIZED)\")\nprint(\"=\" * 80)\n\nclass Config:\n    MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\n    TRAIN_SIZE = 1000   # Reduced slightly to ensure for Kaggle limits\n    VAL_SIZE = 200\n    RANDOM_SEED = 42\n    LORA_R = 16\n    LORA_ALPHA = 32\n    \n    # --- MEMORY OPTIMIZATIONS ---\n    BATCH_SIZE = 1          # Process 1 sample at a time\n    GRAD_ACC = 8           # Accumulate 16 steps to simulate Batch Size 16\n    MAX_SEQ_LENGTH = 300    # Reduced to save VRAM\n    LEARNING_RATE = 2e-4\n    OUTPUT_DIR = \"./models/teacher_mistral_adapters\"\n\nconfig = Config()\n\n# --- 1. DATA ---\nprint(\"Loading and formatting data...\")\ndataset = load_dataset(\"imdb\")\ntrain_data = dataset['train'].shuffle(seed=config.RANDOM_SEED).select(range(config.TRAIN_SIZE))\nval_data = dataset['test'].shuffle(seed=config.RANDOM_SEED).select(range(config.VAL_SIZE))\n\ntokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" \n\ndef format_and_tokenize(example):\n    # Truncate raw text to fit\n    text_clean = example['text'][:1000] \n    label = \"positive\" if example['label'] == 1 else \"negative\"\n    \n    full_text = (\n        f\"[INST] Sentiment Analysis. Return 'positive' or 'negative'.\\n\"\n        f\"Review: {text_clean} [/INST] \\n\"\n        f\"Sentiment: {label}\"\n    )\n    \n    tokenized = tokenizer(\n        full_text,\n        truncation=True,\n        max_length=config.MAX_SEQ_LENGTH,\n        padding=\"max_length\"\n    )\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\ntrain_dataset = train_data.map(format_and_tokenize)\nval_dataset = val_data.map(format_and_tokenize)\n\n# --- 2. MODEL (4-bit + Gradient Checkpointing) ---\nprint(f\"Loading base model: {config.MODEL_NAME}\")\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.MODEL_NAME,\n    quantization_config=bnb_config,\n    device_map=\"auto\", # Let Accelerate handle placement\n    use_cache=False\n)\n\n# ENABLE GRADIENT CHECKPOINTING\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\n# --- 3. LORA ---\npeft_config = LoraConfig(\n    r=config.LORA_R,\n    lora_alpha=config.LORA_ALPHA,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    task_type=TaskType.CAUSAL_LM,\n    bias=\"none\",\n    lora_dropout=0.05,\n)\n\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\n# --- 4. TRAINING ---\ntraining_args = TrainingArguments(\n    output_dir=config.OUTPUT_DIR,\n    num_train_epochs=1,\n    per_device_train_batch_size=config.BATCH_SIZE,\n    gradient_accumulation_steps=config.GRAD_ACC,\n    learning_rate=config.LEARNING_RATE,\n    logging_steps=10,\n    fp16=True,\n    optim=\"paged_adamw_8bit\", # Saves optimizer memory\n    save_strategy=\"no\",\n    report_to=\"none\",\n    ddp_find_unused_parameters=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)\n\nprint(\"Starting Memory-Optimized Fine-Tuning...\")\ntrainer.train()\n\n# --- 5. SAVING ---\nprint(f\"Saving adapters to {config.OUTPUT_DIR}...\")\ntrainer.model.save_pretrained(config.OUTPUT_DIR)\ntokenizer.save_pretrained(config.OUTPUT_DIR)\n\n# CLEANUP FOR NEXT STEPS\ndel model, trainer\ngc.collect()\ntorch.cuda.empty_cache()\nprint(\"✅ Mistral fine-tuning complete & Memory cleared.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:57:22.412579Z","iopub.execute_input":"2025-12-02T16:57:22.412825Z","iopub.status.idle":"2025-12-02T17:25:48.467623Z","shell.execute_reply.started":"2025-12-02T16:57:22.412801Z","shell.execute_reply":"2025-12-02T17:25:48.466793Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nSTEP 3: FINE-TUNING MISTRAL (MEMORY OPTIMIZED)\n================================================================================\nLoading and formatting data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"880f8c51167444daad8224a1cd58a324"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b22676fe23d41efac6002b9de52034a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e431cbe98794217aa0862c30ce8dd5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5d08d37f09d46c9bdfe66dd3949b998"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"670ecee560664973bc1001bf452d23fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fc6b34b60f54b3d96a8cec0c845969e"}},"metadata":{}},{"name":"stdout","text":"Loading base model: mistralai/Mistral-7B-v0.1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a627258c1348f3a4b9e09ad0212a92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b25ccf5b0c4ef883c4fc3fa64b7c3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0da48a8f032f4483a343ce65de508f85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08bab1d6eb214b459ed41afe2efcd05d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607dbed0d2f24271aa15b73c7578100b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed836a51a5ef4257b01d20eb9038d550"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2dbe7480fac4196a7a76cf86c7cc562"}},"metadata":{}},{"name":"stdout","text":"trainable params: 13,631,488 || all params: 7,255,363,584 || trainable%: 0.1879\nStarting Memory-Optimized Fine-Tuning...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 26:38, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.353700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.162800</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.135600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.219500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.166900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.137500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.145100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.105300</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.225600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.166400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>2.110600</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.167900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saving adapters to ./models/teacher_mistral_adapters...\n✅ Mistral fine-tuning complete & Memory cleared.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# FILE: 04a_distill_from_roberta.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    DataCollatorWithPadding,\n    get_linear_schedule_with_warmup\n)\nfrom datasets import load_dataset\nfrom accelerate import Accelerator\nfrom tqdm.auto import tqdm\nimport os\n\nprint(\"=\" * 80)\nprint(\"STEP 4a: DISTILLATION (ROBERTA -> DISTILBERT)\")\nprint(\"LOGITS + FEATURES + ATTENTION\")\nprint(\"=\" * 80)\n\n# --- CONFIGURATION ---\nclass Config:\n    TEACHER_PATH = \"./models/teacher_roberta\"\n    STUDENT_NAME = \"distilbert-base-uncased\"\n    OUTPUT_DIR = \"./models/student_distilled_from_roberta\"\n    MAX_LENGTH = 256\n    BATCH_SIZE = 32\n    NUM_EPOCHS = 3\n    LEARNING_RATE = 5e-5\n    \n    # Distillation Hyperparameters\n    TEMP = 2.0           # Temperature for softening logits\n    ALPHA_CE = 0.5       # Weight for Hard Labels (Ground Truth)\n    ALPHA_KD = 0.5       # Weight for Soft Labels (Teacher Logits)\n    ALPHA_FEAT = 0.3     # Weight for Hidden State matching\n    \n    # Layer Mapping: DistilBERT(6 layers) -> RoBERTa(24 layers)\n    # Map every student layer to every 4th teacher layer\n    # Format: {student_layer_index: teacher_layer_index}\n    LAYER_MAPPING = {0: 3, 1: 7, 2: 11, 3: 15, 4: 19, 5: 23}\n\nconfig = Config()\n\n# --- 1. DATA PREPARATION ---\nprint(\"Loading and tokenizing data...\")\ndataset = load_dataset(\"imdb\")\ntokenizer = AutoTokenizer.from_pretrained(config.STUDENT_NAME)\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, max_length=config.MAX_LENGTH)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\ntokenized_datasets.set_format(\"torch\")\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"], shuffle=True, batch_size=config.BATCH_SIZE, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"test\"], batch_size=config.BATCH_SIZE, collate_fn=data_collator\n)\n\n# --- 2. MODEL DEFINITION ---\nprint(\"Loading models...\")\n\n# Teacher: RoBERTa Large (Frozen)\nteacher_model = AutoModelForSequenceClassification.from_pretrained(\n    config.TEACHER_PATH,\n    num_labels=2,\n    output_hidden_states=True,\n    output_attentions=True\n)\nteacher_model.eval()\nfor param in teacher_model.parameters():\n    param.requires_grad = False\n\n# Student: DistilBERT (Trainable)\n# We wrap it to handle the projection layer for feature matching\nclass DistillableStudent(nn.Module):\n    def __init__(self, student_name, teacher_hidden_size):\n        super().__init__()\n        self.student = AutoModelForSequenceClassification.from_pretrained(\n            student_name, \n            num_labels=2, \n            output_hidden_states=True, \n            output_attentions=True\n        )\n        self.student_hidden_size = self.student.config.hidden_size\n        \n        # Projection Layer: Maps Student (768) -> Teacher (1024)\n        # Create one projection layer per mapped layer to allow specific adaptation\n        self.projections = nn.ModuleList([\n            nn.Linear(self.student_hidden_size, teacher_hidden_size)\n            for _ in range(len(config.LAYER_MAPPING))\n        ])\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        return self.student(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n\nstudent_wrapper = DistillableStudent(config.STUDENT_NAME, teacher_model.config.hidden_size)\n\n# --- 3. TRAINING SETUP (ACCELERATOR) ---\naccelerator = Accelerator()\noptimizer = torch.optim.AdamW(student_wrapper.parameters(), lr=config.LEARNING_RATE)\n\nnum_training_steps = config.NUM_EPOCHS * len(train_dataloader)\nlr_scheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=100, num_training_steps=num_training_steps\n)\n\n# Prepare everything with Accelerator\nstudent_wrapper, teacher_model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n    student_wrapper, teacher_model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n)\n\n# --- 4. LOSS FUNCTION ---\ndef compute_distillation_loss(student_outputs, teacher_outputs, batch, projections):\n    # 1. Hard Label Loss (Cross Entropy with Ground Truth)\n    loss_ce = student_outputs.loss\n    \n    # 2. Soft Label Loss (KL Divergence with Teacher Logits)\n    loss_kd = F.kl_div(\n        F.log_softmax(student_outputs.logits / config.TEMP, dim=-1),\n        F.softmax(teacher_outputs.logits / config.TEMP, dim=-1),\n        reduction=\"batchmean\"\n    ) * (config.TEMP ** 2)\n    \n    # 3. Feature Loss (Hidden State Matching)\n    # Iterate through our mapping: Student Layer i -> Teacher Layer j\n    loss_feat = 0\n    # hidden_states[0] is embeddings, Start from index 1 for layers\n    s_hiddens = student_outputs.hidden_states[1:] \n    t_hiddens = teacher_outputs.hidden_states[1:]\n    \n    for idx, (s_layer_idx, t_layer_idx) in enumerate(config.LAYER_MAPPING.items()):\n        # Project Student (Batch, Seq, 768) -> (Batch, Seq, 1024)\n        s_feat = projections[idx](s_hiddens[s_layer_idx])\n        t_feat = t_hiddens[t_layer_idx]\n        loss_feat += F.mse_loss(s_feat, t_feat)\n    \n    loss_feat = loss_feat / len(config.LAYER_MAPPING)\n\n    # Total Loss\n    return (config.ALPHA_CE * loss_ce) + (config.ALPHA_KD * loss_kd) + (config.ALPHA_FEAT * loss_feat)\n\n# --- 5. TRAINING LOOP ---\nprint(\"Starting distillation...\")\n\nfor epoch in range(config.NUM_EPOCHS):\n    student_wrapper.train()\n    total_loss = 0\n    \n    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", disable=not accelerator.is_local_main_process)\n    \n    for batch in progress_bar:\n        # Forward Pass Teacher (No Grad)\n        with torch.no_grad():\n            teacher_outputs = teacher_model(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"]\n            )\n        \n        # Forward Pass Student\n        student_outputs = student_wrapper(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=batch[\"labels\"]\n        )\n        \n        # Calculate Loss\n        loss = compute_distillation_loss(\n            student_outputs, \n            teacher_outputs, \n            batch, \n            student_wrapper.projections\n        )\n        \n        # Backward\n        optimizer.zero_grad()\n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        \n        total_loss += loss.item()\n        progress_bar.set_postfix({\"loss\": loss.item()})\n    \n    avg_train_loss = total_loss / len(train_dataloader)\n    if accelerator.is_main_process:\n        print(f\"Epoch {epoch+1} | Average Train Loss: {avg_train_loss:.4f}\")\n\n    # --- EVALUATION ---\n    student_wrapper.eval()\n    correct = 0\n    total = 0\n    \n    for batch in eval_dataloader:\n        with torch.no_grad():\n            outputs = student_wrapper(\n                input_ids=batch[\"input_ids\"],\n                attention_mask=batch[\"attention_mask\"]\n            )\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        correct += (predictions == batch[\"labels\"]).sum().item()\n        total += batch[\"labels\"].size(0)\n    \n    acc = correct / total\n    if accelerator.is_main_process:\n        print(f\"Epoch {epoch+1} | Validation Accuracy: {acc:.4f}\")\n\n# --- 6. SAVING ---\nif accelerator.is_main_process:\n    print(f\"\\nSaving distilled model to {config.OUTPUT_DIR}...\")\n    os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n    \n    # We unwrap the model to save just the DistilBERT part, not the projections\n    # (Projections are only needed for training)\n    unwrapped_wrapper = accelerator.unwrap_model(student_wrapper)\n    unwrapped_wrapper.student.save_pretrained(config.OUTPUT_DIR)\n    tokenizer.save_pretrained(config.OUTPUT_DIR)\n    print(\"✅ Distillation complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T17:25:48.469898Z","iopub.execute_input":"2025-12-02T17:25:48.470302Z","iopub.status.idle":"2025-12-02T18:05:49.397743Z","shell.execute_reply.started":"2025-12-02T17:25:48.470270Z","shell.execute_reply":"2025-12-02T18:05:49.397077Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nSTEP 4a: DISTILLATION (ROBERTA -> DISTILBERT)\nLOGITS + FEATURES + ATTENTION\n================================================================================\nLoading and tokenizing data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64defedca3fe49838d1c066b3db4ebc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2a1f91a74254730a528c9787959bfe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d968b0f37be4f258ea3c0c3635202d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdcaa1856b2d40649a85cf5561b61da2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eef6ef21d0af4c579c62e1d990f5797d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f44d55b1081b49788a2a938d3f717273"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa7f4789037944cdb1ab4fa8603ef488"}},"metadata":{}},{"name":"stdout","text":"Loading models...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2545469bf5b14edebf97d0c0bab238c0"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Starting distillation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3707387fa1754c489736dec18463e48f"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 | Average Train Loss: 0.5200\nEpoch 1 | Validation Accuracy: 0.8392\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b8163bf85ef4d0f9c4b5361b465b58d"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 | Average Train Loss: 0.4146\nEpoch 2 | Validation Accuracy: 0.8075\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ba577faf117484a8b2d008ad6a5dff9"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 | Average Train Loss: 0.3921\nEpoch 3 | Validation Accuracy: 0.8748\n\nSaving distilled model to ./models/student_distilled_from_roberta...\n✅ Distillation complete.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# FILE: 04b_distill_from_mistral.py\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom peft import PeftModel, PeftConfig\nfrom datasets import load_dataset, Dataset\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nprint(\"=\" * 80)\nprint(\"STEP 4b: DISTILLING MISTRAL (GENERATIVE) -> DISTILBERT (CLASSIFIER)\")\nprint(\"=\" * 80)\n\n# --- CONFIG ---\nMISTRAL_ADAPTER_PATH = \"./models/teacher_mistral_adapters\"\nSTUDENT_NAME = \"distilbert-base-uncased\"\nOUTPUT_DIR = \"./models/student_distilled_from_mistral\"\nCACHE_FILE = \"mistral_soft_labels.csv\"\n\n# --- PART 1: GENERATE SOFT LABELS WITH MISTRAL ---\nif not os.path.exists(CACHE_FILE):\n    print(\"Generating soft labels from Mistral (this takes time)...\")\n    \n    # Load Base + Adapter\n    config = PeftConfig.from_pretrained(MISTRAL_ADAPTER_PATH)\n    base_model = AutoModelForCausalLM.from_pretrained(\n        config.base_model_name_or_path,\n        load_in_4bit=True,\n        device_map=\"auto\"\n    )\n    teacher_model = PeftModel.from_pretrained(base_model, MISTRAL_ADAPTER_PATH)\n    teacher_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n    teacher_tokenizer.pad_token = teacher_tokenizer.eos_token\n    \n    # Identify token IDs for \"positive\" and \"negative\"\n    # Note: Tokenization depends on leading spaces. \n    pos_id = teacher_tokenizer.encode(\"positive\", add_special_tokens=False)[0]\n    neg_id = teacher_tokenizer.encode(\"negative\", add_special_tokens=False)[0]\n    print(f\"Token IDs - Positive: {pos_id}, Negative: {neg_id}\")\n\n    dataset = load_dataset(\"imdb\", split=\"train[:2000]\") # Subset for demo speed\n    \n    results = []\n    \n    for item in tqdm(dataset):\n        text = item['text'][:1000] # Truncate for speed\n        prompt = f\"[INST] Analyze the sentiment. Return 'positive' or 'negative'.\\nReview: {text} [/INST] \\nSentiment:\"\n        \n        inputs = teacher_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        \n        with torch.no_grad():\n            outputs = teacher_model(**inputs)\n            # Get logits of the last token\n            last_token_logits = outputs.logits[0, -1, :]\n            \n            # Extract logits for specific target words\n            score_pos = last_token_logits[pos_id].item()\n            score_neg = last_token_logits[neg_id].item()\n            \n        results.append({\n            \"text\": item['text'],\n            \"gt_label\": item['label'],\n            \"teacher_logit_pos\": score_pos,\n            \"teacher_logit_neg\": score_neg\n        })\n        \n    df = pd.DataFrame(results)\n    df.to_csv(CACHE_FILE, index=False)\n    # Clean up GPU\n    del teacher_model, base_model\n    torch.cuda.empty_cache()\nelse:\n    print(f\"Loading cached soft labels from {CACHE_FILE}\")\n    df = pd.read_csv(CACHE_FILE)\n\n# --- PART 2: TRAIN STUDENT (DISTILBERT) ---\nprint(\"Training Student on Mistral's Soft Labels...\")\n\n# Convert logits to probabilities (Softmax)\n# [neg_logit, pos_logit] as the two class logits\nteacher_logits = df[['teacher_logit_neg', 'teacher_logit_pos']].values\nprobs = torch.nn.functional.softmax(torch.tensor(teacher_logits, dtype=torch.float32), dim=1)\n\n# Create dataset\ntrain_ds = Dataset.from_dict({\n    \"text\": df['text'].tolist(),\n    \"label\": df['gt_label'].tolist(),\n    \"teacher_probs\": probs.tolist()\n})\n\ntokenizer = AutoTokenizer.from_pretrained(STUDENT_NAME)\ndef tokenize(batch):\n    return tokenizer(batch['text'], padding=\"max_length\", truncation=True, max_length=256)\n\ntrain_ds = train_ds.map(tokenize, batched=True)\n\n# Custom Trainer to handle Soft Labels\nclass DistillationTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs): # Fixed signature\n        labels = inputs.get(\"labels\")\n        teacher_probs = inputs.pop(\"teacher_probs\") # Extract custom column\n        \n        outputs = model(**inputs)\n        student_logits = outputs.logits\n        \n        # KL Divergence Loss\n        # Student LogSoftmax vs Teacher Probs\n        loss_kd = F.kl_div(\n            F.log_softmax(student_logits, dim=-1),\n            teacher_probs,\n            reduction=\"batchmean\"\n        )\n        \n        # Standard Cross Entropy with Ground Truth\n        loss_ce = F.cross_entropy(student_logits, labels)\n        \n        # Weighted Sum\n        loss = 0.5 * loss_kd + 0.5 * loss_ce\n        \n        return (loss, outputs) if return_outputs else loss\n\nstudent_model = AutoModelForSequenceClassification.from_pretrained(STUDENT_NAME, num_labels=2)\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    learning_rate=2e-5,\n    remove_unused_columns=False, # For'teacher_probs' not be dropped\n    report_to=\"none\"\n)\n\ntrainer = DistillationTrainer(\n    model=student_model,\n    args=training_args,\n    train_dataset=train_ds,\n)\n\ntrainer.train()\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(\"✅ Mistral -> DistilBERT Distillation Complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:05:49.398546Z","iopub.execute_input":"2025-12-02T18:05:49.398874Z","iopub.status.idle":"2025-12-02T19:11:59.499491Z","shell.execute_reply.started":"2025-12-02T18:05:49.398852Z","shell.execute_reply":"2025-12-02T19:11:59.498752Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nSTEP 4b: DISTILLING MISTRAL (GENERATIVE) -> DISTILBERT (CLASSIFIER)\n================================================================================\nGenerating soft labels from Mistral (this takes time)...\n","output_type":"stream"},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e16525857fb4574980d1e5f1d78afd5"}},"metadata":{}},{"name":"stdout","text":"Token IDs - Positive: 5278, Negative: 7087\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2000/2000 [1:03:29<00:00,  1.90s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training Student on Mistral's Soft Labels...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"582a1c021188468cb68b3dc2d4f7e057"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [189/189 01:27, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"✅ Mistral -> DistilBERT Distillation Complete.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# FILE: 05_comparison.py\nimport pandas as pd\nimport numpy as np\nimport torch\nimport time\nimport os\nimport gc\nimport joblib\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    AutoModelForCausalLM, \n    BitsAndBytesConfig\n)\nfrom peft import PeftModel, PeftConfig\n\nprint(\"=\" * 80)\nprint(\"STEP 5: COMPREHENSIVE BENCHMARK (TEACHERS VS STUDENTS)\")\nprint(\"=\" * 80)\n\n# --- CONFIGURATION ---\nclass BenchConfig:\n    # Paths\n    PATH_BASELINE = \"./models/baseline_model.joblib\"\n    PATH_TEACHER_ROBERTA = \"./models/teacher_roberta\"\n    PATH_TEACHER_MISTRAL = \"./models/teacher_mistral_adapters\"\n    PATH_STUDENT_ROBERTA = \"./models/student_distilled_from_roberta\"\n    PATH_STUDENT_MISTRAL = \"./models/student_distilled_from_mistral\"\n    \n    # Settings\n    TEST_SUBSET_SIZE = 500  # Keep small for Kaggle execution time\n    LATENCY_SAMPLES = 50    # Samples to measure single-item latency\n    BATCH_SIZE = 16         # For throughput measurement\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nconfig = BenchConfig()\n\n# --- UTILS ---\ndef get_model_size_mb(path):\n    \"\"\"Calculates directory size in MB\"\"\"\n    if os.path.isfile(path):\n        return os.path.getsize(path) / (1024 * 1024)\n    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            total_size += os.path.getsize(fp)\n    return total_size / (1024 * 1024)\n\ndef clean_memory():\n    \"\"\"Aggressively clears GPU memory\"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# --- DATA LOADING ---\nprint(\"Loading Test Data...\")\ndataset = load_dataset(\"imdb\", split=\"test\")\n# Shuffle and select subset to save time\ndataset = dataset.shuffle(seed=42).select(range(config.TEST_SUBSET_SIZE))\ntexts = dataset['text']\nlabels = dataset['label']\n\nresults = []\n\n# ==========================================\n# 1. BENCHMARK BASELINE\n# ==========================================\ntry:\n    print(\"\\n--- Benchmarking Baseline (TF-IDF) ---\")\n    model = joblib.load(config.PATH_BASELINE)\n    \n    # Latency (CPU)\n    start = time.time()\n    for i in range(config.LATENCY_SAMPLES):\n        _ = model.predict([texts[i]])\n    avg_latency = ((time.time() - start) / config.LATENCY_SAMPLES) * 1000\n    \n    # Throughput\n    start = time.time()\n    preds = model.predict(texts)\n    total_time = time.time() - start\n    throughput = len(texts) / total_time\n    \n    # Accuracy\n    acc = accuracy_score(labels, preds)\n    \n    results.append({\n        \"Model\": \"Baseline (TF-IDF)\",\n        \"Type\": \"Traditional\",\n        \"Params (M)\": 0, # Negligible\n        \"Size (MB)\": get_model_size_mb(config.PATH_BASELINE),\n        \"Accuracy\": acc,\n        \"Latency (ms)\": avg_latency,\n        \"Throughput (samp/s)\": throughput\n    })\n    del model\nexcept Exception as e:\n    print(f\"Skipped Baseline: {e}\")\n\n# ==========================================\n# 2. BENCHMARK TRANSFORMER CLASSIFIERS\n# (RoBERTa Teacher, DistilBERT Students)\n# ==========================================\ndef benchmark_classifier(model_path, model_name, model_type):\n    print(f\"\\n--- Benchmarking {model_name} ---\")\n    clean_memory()\n    \n    try:\n        tokenizer = AutoTokenizer.from_pretrained(model_path)\n        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n        model.to(config.DEVICE).eval()\n        \n        # Calculate Params\n        num_params = sum(p.numel() for p in model.parameters()) / 1_000_000\n        \n        # 1. Latency (Batch size 1)\n        latencies = []\n        for i in range(config.LATENCY_SAMPLES):\n            inputs = tokenizer(texts[i], return_tensors=\"pt\", truncation=True, max_length=256).to(config.DEVICE)\n            start = time.time()\n            with torch.no_grad():\n                _ = model(**inputs)\n            latencies.append((time.time() - start) * 1000)\n        avg_latency = np.mean(latencies)\n        \n        # 2. Accuracy & Throughput (Batched)\n        all_preds = []\n        start_time = time.time()\n        \n        for i in range(0, len(texts), config.BATCH_SIZE):\n            batch_texts = texts[i : i + config.BATCH_SIZE]\n            inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(config.DEVICE)\n            \n            with torch.no_grad():\n                outputs = model(**inputs)\n                preds = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n                all_preds.extend(preds)\n                \n        total_time = time.time() - start_time\n        throughput = len(texts) / total_time\n        acc = accuracy_score(labels, all_preds)\n        \n        results.append({\n            \"Model\": model_name,\n            \"Type\": model_type,\n            \"Params (M)\": num_params,\n            \"Size (MB)\": get_model_size_mb(model_path),\n            \"Accuracy\": acc,\n            \"Latency (ms)\": avg_latency,\n            \"Throughput (samp/s)\": throughput\n        })\n        \n        del model, tokenizer\n        clean_memory()\n        \n    except Exception as e:\n        print(f\"Failed to benchmark {model_name}: {e}\")\n\n# Run for Classifiers\nbenchmark_classifier(config.PATH_TEACHER_ROBERTA, \"Teacher (RoBERTa-L)\", \"Teacher\")\nbenchmark_classifier(config.PATH_STUDENT_ROBERTA, \"Student (from RoBERTa)\", \"Student\")\nbenchmark_classifier(config.PATH_STUDENT_MISTRAL, \"Student (from Mistral)\", \"Student\")\n\n# ==========================================\n# 3. BENCHMARK MISTRAL (GENERATIVE)\n# ==========================================\nprint(f\"\\n--- Benchmarking Teacher (Mistral-7B) ---\")\nclean_memory()\n\ntry:\n    # Load Adapter Config\n    peft_config = PeftConfig.from_pretrained(config.PATH_TEACHER_MISTRAL)\n    \n    # Load Base Model (4-bit)\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16\n    )\n    \n    base_model = AutoModelForCausalLM.from_pretrained(\n        peft_config.base_model_name_or_path,\n        quantization_config=bnb_config,\n        device_map=\"auto\"\n    )\n    \n    # Load Adapter\n    model = PeftModel.from_pretrained(base_model, config.PATH_TEACHER_MISTRAL)\n    tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    # Params (Base + Adapter)\n    num_params = sum(p.numel() for p in model.parameters()) / 1_000_000 # This counts all params (even frozen)\n    \n    # Helper for Prompting\n    def format_prompt(text):\n        return f\"[INST] Analyze the sentiment. Return 'positive' or 'negative'.\\nReview: {text[:1000]} [/INST] \\nSentiment:\"\n\n    # 1. Latency (Generation is slow, do fewer samples)\n    latencies = []\n    for i in range(10): # Only 10 samples for Mistral Latency to save time\n        prompt = format_prompt(texts[i])\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(config.DEVICE)\n        \n        start = time.time()\n        with torch.no_grad():\n            _ = model.generate(**inputs, max_new_tokens=2, pad_token_id=tokenizer.eos_token_id)\n        latencies.append((time.time() - start) * 1000)\n    avg_latency = np.mean(latencies)\n\n    # 2. Accuracy & Throughput\n    # Batch size 1 for simplicity and correctness in this script, \n    # acknowledging this penalizes Mistral's throughput slightly.\n    \n    correct = 0\n    start_time = time.time()\n    \n    # Limit Mistral evaluation to 100 samples\n    eval_limit = 100 \n    print(f\"  (Evaluating Mistral on first {eval_limit} samples only...)\")\n    \n    for i in tqdm(range(eval_limit)):\n        prompt = format_prompt(texts[i])\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(config.DEVICE)\n        \n        with torch.no_grad():\n            outputs = model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n        \n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n        \n        # Simple parsing\n        prediction = 1 if \"positive\" in generated_text.split(\"sentiment:\")[-1] else 0\n        if prediction == labels[i]:\n            correct += 1\n            \n    total_time = time.time() - start_time\n    throughput = eval_limit / total_time\n    acc = correct / eval_limit\n    \n    results.append({\n        \"Model\": \"Teacher (Mistral-7B)\",\n        \"Type\": \"Teacher\",\n        \"Params (M)\": 7000, # Approx\n        \"Size (MB)\": 15000, # Approx 4-bit size\n        \"Accuracy\": acc,\n        \"Latency (ms)\": avg_latency,\n        \"Throughput (samp/s)\": throughput\n    })\n    \n    del model, base_model, tokenizer\n    clean_memory()\n\nexcept Exception as e:\n    print(f\"Failed to benchmark Mistral: {e}\")\n\n# ==========================================\n# 4. FINAL REPORT\n# ==========================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"FINAL BENCHMARK REPORT\")\nprint(\"=\"*80)\n\ndf = pd.DataFrame(results)\ndf = df.set_index(\"Model\")\n\n# Calculate Improvement Metrics\ntry:\n    rob_lat = df.loc[\"Teacher (RoBERTa-L)\", \"Latency (ms)\"]\n    rob_size = df.loc[\"Teacher (RoBERTa-L)\", \"Size (MB)\"]\n    \n    df[\"Speedup (x)\"] = rob_lat / df[\"Latency (ms)\"]\n    df[\"Size Reduction (x)\"] = rob_size / df[\"Size (MB)\"]\nexcept KeyError:\n    pass\n\n# Reorder columns\ncols = [\"Type\", \"Accuracy\", \"Latency (ms)\", \"Throughput (samp/s)\", \"Speedup (x)\", \"Size (MB)\", \"Params (M)\"]\ndf = df[cols]\n\nprint(df.to_string(float_format=\"%.2f\"))\n\nprint(\"\\n--- Analysis ---\")\nprint(\"1. Latency: Lower is better. Critical for real-time APIs.\")\nprint(\"2. Throughput: Higher is better. Critical for offline batch processing.\")\nprint(\"3. Mistral Note: Generative models are significantly slower than classifiers because\")\nprint(\"   they generate token-by-token. Distilling Mistral -> DistilBERT unlocks massive speedups.\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T19:11:59.500312Z","iopub.execute_input":"2025-12-02T19:11:59.500668Z","iopub.status.idle":"2025-12-02T19:15:25.860934Z","shell.execute_reply.started":"2025-12-02T19:11:59.500643Z","shell.execute_reply":"2025-12-02T19:15:25.860260Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nSTEP 5: COMPREHENSIVE BENCHMARK (TEACHERS VS STUDENTS)\n================================================================================\nLoading Test Data...\n\n--- Benchmarking Baseline (TF-IDF) ---\n\n--- Benchmarking Teacher (RoBERTa-L) ---\n\n--- Benchmarking Student (from RoBERTa) ---\n","output_type":"stream"},{"name":"stderr","text":"DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n","output_type":"stream"},{"name":"stdout","text":"\n--- Benchmarking Student (from Mistral) ---\n\n--- Benchmarking Teacher (Mistral-7B) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab6e29783e4c4a31bcf3ae6261d49621"}},"metadata":{}},{"name":"stdout","text":"  (Evaluating Mistral on first 100 samples only...)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [01:26<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nFINAL BENCHMARK REPORT\n================================================================================\n                               Type  Accuracy  Latency (ms)  Throughput (samp/s)  Speedup (x)  Size (MB)  Params (M)\nModel                                                                                                               \nBaseline (TF-IDF)       Traditional      0.84          1.33              4135.32        12.84      28.80        0.00\nTeacher (RoBERTa-L)         Teacher      0.96         17.11                18.41         1.00    5432.01      355.36\nStudent (from RoBERTa)      Student      0.87          5.91               103.00         2.89     256.33       66.96\nStudent (from Mistral)      Student      0.51          4.07               115.05         4.21    1022.69       66.96\nTeacher (Mistral-7B)        Teacher      0.95        522.36                 1.15         0.03   15000.00     7000.00\n\n--- Analysis ---\n1. Latency: Lower is better. Critical for real-time APIs.\n2. Throughput: Higher is better. Critical for offline batch processing.\n3. Mistral Note: Generative models are significantly slower than classifiers because\n   they generate token-by-token. Distilling Mistral -> DistilBERT unlocks massive speedups.\n================================================================================\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}